{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bcf34e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 10:23:19,418 | INFO | HTTP Request: GET https://blkxb2wttx6xuwtoqqww.c0.asia-southeast1.gcp.weaviate.cloud/v1/meta \"HTTP/1.1 200 OK\"\n",
      "2025-06-11 10:23:19,658 | INFO | HTTP Request: GET https://pypi.org/pypi/weaviate-client/json \"HTTP/1.1 200 OK\"\n",
      "/home/user/gen-ai/lib/python3.12/site-packages/weaviate/warnings.py:314: ResourceWarning: Con004: The connection to Weaviate was not closed properly. This can lead to memory leaks.\n",
      "            Please make sure to close the connection using `client.close()`.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_8220/1476490365.py:40: ResourceWarning: unclosed <ssl.SSLSocket fd=74, family=2, type=1, proto=6, laddr=('10.33.191.12', 54628), raddr=('34.160.174.119', 443)>\n",
      "  collection = weaviate_client.collections.get(collection_name)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "# --- Imports and Setup ---\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "from typing import List, Dict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed, retry_if_exception_type\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "\n",
    "endpoint = \"https://swedencentral.api.cognitive.microsoft.com/\"\n",
    "model_name = \"gpt-4o\"\n",
    "deployment = \"gpt-4o\"\n",
    "\n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY_3\")\n",
    "api_version = \"2024-12-01-preview\"\n",
    "\n",
    "llm_client = AzureOpenAI(\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    ")\n",
    "\n",
    "# Weaviate setup\n",
    "weaviate_url = os.environ[\"WEAVIATE_URL\"]\n",
    "weaviate_api_key = os.environ[\"WEAVIATE_API_KEY\"]\n",
    "collection_name = \"Docs\"\n",
    "\n",
    "weaviate_client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=weaviate_url,\n",
    "    auth_credentials=Auth.api_key(weaviate_api_key),\n",
    ")\n",
    "collection = weaviate_client.collections.get(collection_name)\n",
    "\n",
    "# --- Retry Decorators ---\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_fixed(2), retry=retry_if_exception_type(Exception))\n",
    "def safe_llm(prompt: str, temperature: float = 0) -> str:\n",
    "    logging.info(f\"Calling LLM with prompt: {prompt[:100]}...\")\n",
    "    completion = llm_client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=512,\n",
    "        stop=None\n",
    "    )\n",
    "    return completion.choices[0].message.content.strip()\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_fixed(2), retry=retry_if_exception_type(Exception))\n",
    "def safe_weaviate_query(query_vector, limit):\n",
    "    logging.info(\"Querying Weaviate for KB hits.\")\n",
    "    results = collection.query.near_vector(\n",
    "        near=query_vector,\n",
    "        limit=limit,\n",
    "        return_metadata=[\"doc_id\", \"source\", \"last_updated\", \"answer_snippet\"],\n",
    "        return_properties=[\"doc_id\", \"source\", \"last_updated\", \"answer_snippet\"],\n",
    "    )\n",
    "    return results\n",
    "\n",
    "# --- Embedding (for retriever/refiner nodes) ---\n",
    "from openai import OpenAI\n",
    "\n",
    "embedding_client = OpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    base_url=os.getenv(\"MODEL_BASE_URL\"),\n",
    "    default_headers={\"api-key\": os.getenv(\"AZURE_OPENAI_API_KEY\")},\n",
    "    default_query={\"api-version\": os.getenv(\"AZURE_OPENAI_API_VERSION\")}\n",
    ")\n",
    "\n",
    "def get_embedding(text):\n",
    "    logging.info(f\"Embedding text for retrieval: {text[:80]}\")\n",
    "    response = embedding_client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# --- Nodes ---\n",
    "def retrieve_kb_node(state: dict) -> dict:\n",
    "    user_question = state[\"user_question\"]\n",
    "    try:\n",
    "        query_vector = get_embedding(user_question)\n",
    "        results = safe_weaviate_query(query_vector, 5)\n",
    "        kb_hits = []\n",
    "        for res in results.objects:\n",
    "            kb_hits.append({\n",
    "                \"doc_id\": res.properties.get(\"doc_id\"),\n",
    "                \"answer_snippet\": res.properties.get(\"answer_snippet\", \"\"),\n",
    "                \"source\": res.properties.get(\"source\", \"\"),\n",
    "            })\n",
    "        logging.info(f\"KB Hits: {[h['doc_id'] for h in kb_hits]}\")\n",
    "        return {**state, \"kb_hits\": kb_hits}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"KB retrieval failed: {e}\")\n",
    "        return {**state, \"kb_hits\": []}\n",
    "\n",
    "def generate_answer_node(state: dict) -> dict:\n",
    "    user_question = state[\"user_question\"]\n",
    "    kb_hits = state[\"kb_hits\"]\n",
    "    kb_snippets = \"\\n\".join(\n",
    "        f\"[{hit['doc_id']}] {hit['answer_snippet']}\" for hit in kb_hits\n",
    "    )\n",
    "    prompt = f\"\"\"\n",
    "You are a software best-practices assistant.\n",
    "User Question:\n",
    "{user_question}\n",
    "Retrieved Snippets:\n",
    "{kb_snippets}\n",
    "Task:\n",
    "Based on these snippets, write a concise answer to the userâ€™s question.\n",
    "Cite each snippet you use by its doc_id in square brackets (e.g., [KB004]).\n",
    "Return only the answer text.\n",
    "\"\"\"\n",
    "    initial_answer = safe_llm(prompt)\n",
    "    logging.info(f\"Initial answer: {initial_answer}\")\n",
    "    return {**state, \"initial_answer\": initial_answer}\n",
    "\n",
    "def critique_answer_node(state: dict) -> dict:\n",
    "    user_question = state[\"user_question\"]\n",
    "    initial_answer = state[\"initial_answer\"]\n",
    "    kb_hits = state[\"kb_hits\"]\n",
    "    kb_snippets = \"\\n\".join(\n",
    "        f\"[{hit['doc_id']}] {hit['answer_snippet']}\" for hit in kb_hits\n",
    "    )\n",
    "    prompt = f\"\"\"\n",
    "You are a critical QA assistant. The user asked: {user_question}\n",
    "Initial Answer:\n",
    "{initial_answer}\n",
    "KB Snippets:\n",
    "{kb_snippets}\n",
    "Task:\n",
    "Determine if the initial answer fully addresses the question using only these snippets.\n",
    "- If it does, respond exactly: COMPLETE\n",
    "- If it misses any point or cites missing info, respond: REFINE: <short list of missing topic keywords>\n",
    "Return exactly one line.\n",
    "\"\"\"\n",
    "    critique_result = safe_llm(prompt)\n",
    "    logging.info(f\"Critique result: {critique_result}\")\n",
    "    return {**state, \"critique_result\": critique_result}\n",
    "\n",
    "def refine_answer_node(state: dict) -> dict:\n",
    "    user_question = state[\"user_question\"]\n",
    "    initial_answer = state[\"initial_answer\"]\n",
    "    critique_result = state[\"critique_result\"]\n",
    "\n",
    "    # Extract missing-topic keywords from critique\n",
    "    match = re.match(r\"REFINE:\\s*(.*)\", critique_result, re.IGNORECASE)\n",
    "    missing_keywords = match.group(1).strip() if match else \"\"\n",
    "    new_query = f\"{user_question} and information on {missing_keywords}\" if missing_keywords else user_question\n",
    "\n",
    "    # Retrieve one extra snippet for refinement\n",
    "    try:\n",
    "        extra_vector = get_embedding(new_query)\n",
    "        results = safe_weaviate_query(extra_vector, 1)\n",
    "        if results.objects:\n",
    "            extra_hit = results.objects[0]\n",
    "            extra_hit_doc_id = extra_hit.properties.get(\"doc_id\", \"\")\n",
    "            extra_hit_snippet = extra_hit.properties.get(\"answer_snippet\", \"\")\n",
    "        else:\n",
    "            extra_hit_doc_id, extra_hit_snippet = \"\", \"\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Refine retrieval failed: {e}\")\n",
    "        extra_hit_doc_id, extra_hit_snippet = \"\", \"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a software best-practices assistant refining your answer. The user asked: {user_question}\n",
    "Initial Answer:\n",
    "{initial_answer}\n",
    "Critique: {critique_result}\n",
    "Additional Snippet:\n",
    "[{extra_hit_doc_id}] {extra_hit_snippet}\n",
    "Task:\n",
    "Incorporate this snippet into the answer, covering the missing points.\n",
    "Cite any snippet you use by doc_id in square brackets.\n",
    "Return only the final refined answer.\n",
    "\"\"\"\n",
    "    refined_answer = safe_llm(prompt)\n",
    "    logging.info(f\"Refined answer: {refined_answer}\")\n",
    "    return {**state, \"refined_answer\": refined_answer}\n",
    "\n",
    "# --- Build the LangGraph graph ---\n",
    "graph = StateGraph(dict)\n",
    "graph.add_node(\"retrieve_kb\", retrieve_kb_node)\n",
    "graph.add_node(\"generate_answer\", generate_answer_node)\n",
    "graph.add_node(\"critique_answer\", critique_answer_node)\n",
    "graph.add_node(\"refine_answer\", refine_answer_node)\n",
    "\n",
    "graph.add_edge(\"retrieve_kb\", \"generate_answer\")\n",
    "graph.add_edge(\"generate_answer\", \"critique_answer\")\n",
    "\n",
    "def critique_decision(state: dict):\n",
    "    critique_result = state.get(\"critique_result\", \"\")\n",
    "    if critique_result.strip().upper() == \"COMPLETE\":\n",
    "        return END\n",
    "    elif critique_result.strip().upper().startswith(\"REFINE\"):\n",
    "        return \"refine_answer\"\n",
    "    else:\n",
    "        return END\n",
    "\n",
    "graph.add_conditional_edges(\"critique_answer\", critique_decision, {\"refine_answer\", END})\n",
    "graph.add_edge(\"refine_answer\", END)\n",
    "graph.set_entry_point(\"retrieve_kb\")\n",
    "\n",
    "# --- Pipeline Driver Function ---\n",
    "def agentic_rag_qa(user_question: str):\n",
    "    initial_state = {\n",
    "        \"user_question\": user_question,\n",
    "        \"kb_hits\": [],\n",
    "        \"initial_answer\": \"\",\n",
    "        \"critique_result\": \"\",\n",
    "        \"refined_answer\": \"\"\n",
    "    }\n",
    "    runnable = graph.compile()\n",
    "    final_state = runnable.invoke(initial_state)\n",
    "\n",
    "    # Output diagnostics\n",
    "    hits = final_state.get(\"kb_hits\", [])\n",
    "    print(f\"KB Hits: {[h['doc_id'] for h in hits]}\")\n",
    "    print(f\"Initial Answer: {final_state.get('initial_answer')}\")\n",
    "    print(f\"Critique: {final_state.get('critique_result')}\")\n",
    "    if final_state.get(\"refined_answer\"):\n",
    "        print(\"Answer was refined.\")\n",
    "        print(f\"Refined Answer: {final_state.get('refined_answer')}\")\n",
    "    else:\n",
    "        print(\"Answer was not refined.\")\n",
    "\n",
    "    answer = final_state.get(\"refined_answer\") or final_state.get(\"initial_answer\")\n",
    "    return {\"answer\": answer}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e507fab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 10:24:18,993 | INFO | Embedding text for retrieval: How should I set up CI/CD pipelines ? \n",
      "2025-06-11 10:24:20,055 | INFO | HTTP Request: POST https://eastus.api.cognitive.microsoft.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-02-01 \"HTTP/1.1 200 OK\"\n",
      "2025-06-11 10:24:20,056 | INFO | Querying Weaviate for KB hits.\n",
      "2025-06-11 10:24:22,057 | INFO | Querying Weaviate for KB hits.\n",
      "2025-06-11 10:24:24,058 | INFO | Querying Weaviate for KB hits.\n",
      "2025-06-11 10:24:24,059 | ERROR | KB retrieval failed: RetryError[<Future at 0x792ec01b4b90 state=finished raised TypeError>]\n",
      "2025-06-11 10:24:24,059 | INFO | Calling LLM with prompt: \n",
      "You are a software best-practices assistant.\n",
      "User Question:\n",
      "How should I set up CI/CD pipelines ? \n",
      "...\n",
      "2025-06-11 10:24:25,999 | INFO | HTTP Request: POST https://swedencentral.api.cognitive.microsoft.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-06-11 10:24:26,002 | INFO | Initial answer: To set up CI/CD pipelines, start by defining clear stages such as build, test, and deploy to ensure code quality and reliability [KB001]. Use version control systems like Git to trigger pipeline workflows automatically upon code changes [KB002]. Incorporate automated testing to catch issues early and ensure consistent deployments [KB003]. Secure your pipeline by managing secrets and access controls effectively [KB005]. Finally, monitor and optimize pipeline performance regularly to adapt to evolving project needs [KB006].\n",
      "2025-06-11 10:24:26,003 | INFO | Calling LLM with prompt: \n",
      "You are a critical QA assistant. The user asked: How should I set up CI/CD pipelines ? \n",
      "Initial Ans...\n",
      "2025-06-11 10:24:26,411 | INFO | HTTP Request: POST https://swedencentral.api.cognitive.microsoft.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-06-11 10:24:26,413 | INFO | Critique result: REFINE: pipeline setup steps\n",
      "2025-06-11 10:24:26,414 | INFO | Embedding text for retrieval: How should I set up CI/CD pipelines ?  and information on pipeline setup steps\n",
      "2025-06-11 10:24:27,391 | INFO | HTTP Request: POST https://eastus.api.cognitive.microsoft.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-02-01 \"HTTP/1.1 200 OK\"\n",
      "2025-06-11 10:24:27,392 | INFO | Querying Weaviate for KB hits.\n",
      "2025-06-11 10:24:29,396 | INFO | Querying Weaviate for KB hits.\n",
      "2025-06-11 10:24:31,397 | INFO | Querying Weaviate for KB hits.\n",
      "2025-06-11 10:24:31,397 | ERROR | Refine retrieval failed: RetryError[<Future at 0x792eaeb85fa0 state=finished raised TypeError>]\n",
      "2025-06-11 10:24:31,398 | INFO | Calling LLM with prompt: \n",
      "You are a software best-practices assistant refining your answer. The user asked: How should I set ...\n",
      "2025-06-11 10:24:33,392 | INFO | HTTP Request: POST https://swedencentral.api.cognitive.microsoft.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-06-11 10:24:33,393 | INFO | Refined answer: To set up CI/CD pipelines, start by defining clear stages such as build, test, and deploy to ensure code quality and reliability [KB001]. Use version control systems like Git to trigger pipeline workflows automatically upon code changes [KB002]. Incorporate automated testing to catch issues early and ensure consistent deployments [KB003]. Secure your pipeline by managing secrets and access controls effectively [KB005]. Use containerization tools like Docker to standardize environments across development, testing, and production stages [KB004]. Implement infrastructure-as-code (IaC) tools such as Terraform or CloudFormation to automate environment provisioning and ensure reproducibility [KB007]. Finally, monitor and optimize pipeline performance regularly to adapt to evolving project needs [KB006].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KB Hits: []\n",
      "Initial Answer: To set up CI/CD pipelines, start by defining clear stages such as build, test, and deploy to ensure code quality and reliability [KB001]. Use version control systems like Git to trigger pipeline workflows automatically upon code changes [KB002]. Incorporate automated testing to catch issues early and ensure consistent deployments [KB003]. Secure your pipeline by managing secrets and access controls effectively [KB005]. Finally, monitor and optimize pipeline performance regularly to adapt to evolving project needs [KB006].\n",
      "Critique: REFINE: pipeline setup steps\n",
      "Answer was refined.\n",
      "Refined Answer: To set up CI/CD pipelines, start by defining clear stages such as build, test, and deploy to ensure code quality and reliability [KB001]. Use version control systems like Git to trigger pipeline workflows automatically upon code changes [KB002]. Incorporate automated testing to catch issues early and ensure consistent deployments [KB003]. Secure your pipeline by managing secrets and access controls effectively [KB005]. Use containerization tools like Docker to standardize environments across development, testing, and production stages [KB004]. Implement infrastructure-as-code (IaC) tools such as Terraform or CloudFormation to automate environment provisioning and ensure reproducibility [KB007]. Finally, monitor and optimize pipeline performance regularly to adapt to evolving project needs [KB006].\n",
      "{'answer': 'To set up CI/CD pipelines, start by defining clear stages such as build, test, and deploy to ensure code quality and reliability [KB001]. Use version control systems like Git to trigger pipeline workflows automatically upon code changes [KB002]. Incorporate automated testing to catch issues early and ensure consistent deployments [KB003]. Secure your pipeline by managing secrets and access controls effectively [KB005]. Use containerization tools like Docker to standardize environments across development, testing, and production stages [KB004]. Implement infrastructure-as-code (IaC) tools such as Terraform or CloudFormation to automate environment provisioning and ensure reproducibility [KB007]. Finally, monitor and optimize pipeline performance regularly to adapt to evolving project needs [KB006].'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"How should I set up CI/CD pipelines ? \"\n",
    "response = agentic_rag_qa(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "07f0f59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 10:25:03,093 | INFO | Embedding text for retrieval: What are performance tuning steps ? \n",
      "2025-06-11 10:25:04,071 | INFO | HTTP Request: POST https://eastus.api.cognitive.microsoft.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-02-01 \"HTTP/1.1 200 OK\"\n",
      "2025-06-11 10:25:04,072 | INFO | Querying Weaviate for KB hits.\n",
      "2025-06-11 10:25:06,073 | INFO | Querying Weaviate for KB hits.\n",
      "2025-06-11 10:25:08,075 | INFO | Querying Weaviate for KB hits.\n",
      "2025-06-11 10:25:08,076 | ERROR | KB retrieval failed: RetryError[<Future at 0x792ec01b3950 state=finished raised TypeError>]\n",
      "2025-06-11 10:25:08,077 | INFO | Calling LLM with prompt: \n",
      "You are a software best-practices assistant.\n",
      "User Question:\n",
      "What are performance tuning steps ? \n",
      "Re...\n",
      "2025-06-11 10:25:09,428 | INFO | HTTP Request: POST https://swedencentral.api.cognitive.microsoft.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-06-11 10:25:09,430 | INFO | Initial answer: Performance tuning steps typically include identifying bottlenecks, optimizing code, improving database queries, caching frequently accessed data, and monitoring system performance. Additionally, profiling tools can be used to analyze resource usage and adjust configurations for better efficiency [KB004].\n",
      "2025-06-11 10:25:09,431 | INFO | Calling LLM with prompt: \n",
      "You are a critical QA assistant. The user asked: What are performance tuning steps ? \n",
      "Initial Answe...\n",
      "2025-06-11 10:25:09,868 | INFO | HTTP Request: POST https://swedencentral.api.cognitive.microsoft.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-06-11 10:25:09,870 | INFO | Critique result: REFINE: hardware optimization, load balancing\n",
      "2025-06-11 10:25:09,870 | INFO | Embedding text for retrieval: What are performance tuning steps ?  and information on hardware optimization, l\n",
      "2025-06-11 10:25:10,804 | INFO | HTTP Request: POST https://eastus.api.cognitive.microsoft.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-02-01 \"HTTP/1.1 200 OK\"\n",
      "2025-06-11 10:25:10,806 | INFO | Querying Weaviate for KB hits.\n",
      "2025-06-11 10:25:12,807 | INFO | Querying Weaviate for KB hits.\n",
      "2025-06-11 10:25:14,808 | INFO | Querying Weaviate for KB hits.\n",
      "2025-06-11 10:25:14,809 | ERROR | Refine retrieval failed: RetryError[<Future at 0x792ec3d011f0 state=finished raised TypeError>]\n",
      "2025-06-11 10:25:14,810 | INFO | Calling LLM with prompt: \n",
      "You are a software best-practices assistant refining your answer. The user asked: What are performa...\n",
      "2025-06-11 10:25:16,300 | INFO | HTTP Request: POST https://swedencentral.api.cognitive.microsoft.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-06-11 10:25:16,302 | INFO | Refined answer: Performance tuning steps typically include identifying bottlenecks, optimizing code, improving database queries, caching frequently accessed data, and monitoring system performance. Additionally, profiling tools can be used to analyze resource usage and adjust configurations for better efficiency [KB004]. Hardware optimization, such as upgrading CPUs, increasing memory, or using faster storage solutions, can also significantly improve performance. Implementing load balancing ensures that workloads are distributed evenly across servers, reducing the risk of overloading any single resource and enhancing system reliability.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KB Hits: []\n",
      "Initial Answer: Performance tuning steps typically include identifying bottlenecks, optimizing code, improving database queries, caching frequently accessed data, and monitoring system performance. Additionally, profiling tools can be used to analyze resource usage and adjust configurations for better efficiency [KB004].\n",
      "Critique: REFINE: hardware optimization, load balancing\n",
      "Answer was refined.\n",
      "Refined Answer: Performance tuning steps typically include identifying bottlenecks, optimizing code, improving database queries, caching frequently accessed data, and monitoring system performance. Additionally, profiling tools can be used to analyze resource usage and adjust configurations for better efficiency [KB004]. Hardware optimization, such as upgrading CPUs, increasing memory, or using faster storage solutions, can also significantly improve performance. Implementing load balancing ensures that workloads are distributed evenly across servers, reducing the risk of overloading any single resource and enhancing system reliability.\n",
      "{'answer': 'Performance tuning steps typically include identifying bottlenecks, optimizing code, improving database queries, caching frequently accessed data, and monitoring system performance. Additionally, profiling tools can be used to analyze resource usage and adjust configurations for better efficiency [KB004]. Hardware optimization, such as upgrading CPUs, increasing memory, or using faster storage solutions, can also significantly improve performance. Implementing load balancing ensures that workloads are distributed evenly across servers, reducing the risk of overloading any single resource and enhancing system reliability.'}\n"
     ]
    }
   ],
   "source": [
    "question = \"What are performance tuning steps ? \"\n",
    "response = agentic_rag_qa(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "198ed916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 10:20:52,827 | INFO | Embedding text for retrieval: how do I version my APIs?\n",
      "2025-06-11 10:20:53,926 | INFO | HTTP Request: POST https://eastus.api.cognitive.microsoft.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-02-01 \"HTTP/1.1 200 OK\"\n",
      "2025-06-11 10:20:53,927 | INFO | Querying Weaviate for KB hits.\n",
      "2025-06-11 10:20:55,928 | INFO | Querying Weaviate for KB hits.\n",
      "2025-06-11 10:20:57,929 | INFO | Querying Weaviate for KB hits.\n",
      "2025-06-11 10:20:57,930 | ERROR | KB retrieval failed: RetryError[<Future at 0x792ec3ef9a30 state=finished raised TypeError>]\n",
      "2025-06-11 10:20:57,931 | INFO | Calling LLM with prompt: \n",
      "You are a software best-practices assistant.\n",
      "User Question:\n",
      "how do I version my APIs?\n",
      "Retrieved Sni...\n",
      "2025-06-11 10:21:00,284 | INFO | HTTP Request: POST https://swedencentral.api.cognitive.microsoft.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-06-11 10:21:00,285 | INFO | Initial answer: To version your APIs, use a clear and consistent strategy such as including the version number in the URL (e.g., `/v1/resource`) or in the request header. Ensure backward compatibility for existing clients and document changes thoroughly. Semantic versioning (e.g., v1.0.0) can help communicate the scope of changes, with major versions indicating breaking changes. Avoid overloading endpoints with multiple versions to maintain simplicity and clarity [KB004], [KB007].\n",
      "2025-06-11 10:21:00,286 | INFO | Calling LLM with prompt: \n",
      "You are a critical QA assistant. The user asked: how do I version my APIs?\n",
      "Initial Answer:\n",
      "To versi...\n",
      "2025-06-11 10:21:00,795 | INFO | HTTP Request: POST https://swedencentral.api.cognitive.microsoft.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-06-11 10:21:00,797 | INFO | Critique result: REFINE: backward compatibility, endpoint overloading\n",
      "2025-06-11 10:21:00,797 | INFO | Embedding text for retrieval: how do I version my APIs? and information on backward compatibility, endpoint ov\n",
      "2025-06-11 10:21:01,726 | INFO | HTTP Request: POST https://eastus.api.cognitive.microsoft.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-02-01 \"HTTP/1.1 200 OK\"\n",
      "2025-06-11 10:21:01,727 | INFO | Querying Weaviate for KB hits.\n",
      "2025-06-11 10:21:03,728 | INFO | Querying Weaviate for KB hits.\n",
      "2025-06-11 10:21:05,729 | INFO | Querying Weaviate for KB hits.\n",
      "2025-06-11 10:21:05,730 | ERROR | Refine retrieval failed: RetryError[<Future at 0x792ec3985f40 state=finished raised TypeError>]\n",
      "2025-06-11 10:21:05,730 | INFO | Calling LLM with prompt: \n",
      "You are a software best-practices assistant refining your answer. The user asked: how do I version ...\n",
      "2025-06-11 10:21:07,826 | INFO | HTTP Request: POST https://swedencentral.api.cognitive.microsoft.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-06-11 10:21:07,827 | INFO | Refined answer: To version your APIs, use a clear and consistent strategy such as including the version number in the URL (e.g., `/v1/resource`) or in the request header. Ensure backward compatibility by maintaining support for older versions as long as feasible, allowing existing clients to continue functioning without disruption. Avoid overloading endpoints with multiple versions by keeping each version isolated and distinct, which simplifies maintenance and reduces confusion. Document changes thoroughly, providing clear migration guides for clients. Semantic versioning (e.g., v1.0.0) can help communicate the scope of changes, with major versions indicating breaking changes. Additionally, consider deprecating outdated versions gradually, providing ample notice and support to clients during the transition period [KB004], [KB007].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KB Hits: []\n",
      "Initial Answer: To version your APIs, use a clear and consistent strategy such as including the version number in the URL (e.g., `/v1/resource`) or in the request header. Ensure backward compatibility for existing clients and document changes thoroughly. Semantic versioning (e.g., v1.0.0) can help communicate the scope of changes, with major versions indicating breaking changes. Avoid overloading endpoints with multiple versions to maintain simplicity and clarity [KB004], [KB007].\n",
      "Critique: REFINE: backward compatibility, endpoint overloading\n",
      "Answer was refined.\n",
      "Refined Answer: To version your APIs, use a clear and consistent strategy such as including the version number in the URL (e.g., `/v1/resource`) or in the request header. Ensure backward compatibility by maintaining support for older versions as long as feasible, allowing existing clients to continue functioning without disruption. Avoid overloading endpoints with multiple versions by keeping each version isolated and distinct, which simplifies maintenance and reduces confusion. Document changes thoroughly, providing clear migration guides for clients. Semantic versioning (e.g., v1.0.0) can help communicate the scope of changes, with major versions indicating breaking changes. Additionally, consider deprecating outdated versions gradually, providing ample notice and support to clients during the transition period [KB004], [KB007].\n",
      "{'answer': 'To version your APIs, use a clear and consistent strategy such as including the version number in the URL (e.g., `/v1/resource`) or in the request header. Ensure backward compatibility by maintaining support for older versions as long as feasible, allowing existing clients to continue functioning without disruption. Avoid overloading endpoints with multiple versions by keeping each version isolated and distinct, which simplifies maintenance and reduces confusion. Document changes thoroughly, providing clear migration guides for clients. Semantic versioning (e.g., v1.0.0) can help communicate the scope of changes, with major versions indicating breaking changes. Additionally, consider deprecating outdated versions gradually, providing ample notice and support to clients during the transition period [KB004], [KB007].'}\n"
     ]
    }
   ],
   "source": [
    "question = \"how do I version my APIs?\"\n",
    "response = agentic_rag_qa(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2a80d65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 10:25:35,779 | INFO | Embedding text for retrieval: How do I version my APIs?\n",
      "2025-06-11 10:25:36,782 | INFO | HTTP Request: POST https://eastus.api.cognitive.microsoft.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-02-01 \"HTTP/1.1 200 OK\"\n",
      "2025-06-11 10:25:36,783 | INFO | Querying Weaviate for KB hits.\n",
      "2025-06-11 10:25:38,784 | INFO | Querying Weaviate for KB hits.\n",
      "2025-06-11 10:25:40,785 | INFO | Querying Weaviate for KB hits.\n",
      "2025-06-11 10:25:40,785 | ERROR | KB retrieval failed: RetryError[<Future at 0x792ec3ef9af0 state=finished raised TypeError>]\n",
      "2025-06-11 10:25:40,786 | INFO | Calling LLM with prompt: \n",
      "You are a software best-practices assistant.\n",
      "User Question:\n",
      "How do I version my APIs?\n",
      "Retrieved Sni...\n",
      "2025-06-11 10:25:42,746 | INFO | HTTP Request: POST https://swedencentral.api.cognitive.microsoft.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-06-11 10:25:42,748 | INFO | Initial answer: To version your APIs, use a clear and consistent strategy such as including the version number in the URL (e.g., `/api/v1/resource`) or in the request header. Ensure backward compatibility for existing clients and document changes thoroughly. Semantic versioning (e.g., major.minor.patch) is recommended to indicate the scope of changes. Major versions should be used for breaking changes, minor for new features, and patch for bug fixes. [KB004] [KB007]\n",
      "2025-06-11 10:25:42,748 | INFO | Calling LLM with prompt: \n",
      "You are a critical QA assistant. The user asked: How do I version my APIs?\n",
      "Initial Answer:\n",
      "To versi...\n",
      "2025-06-11 10:25:43,166 | INFO | HTTP Request: POST https://swedencentral.api.cognitive.microsoft.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-06-11 10:25:43,168 | INFO | Critique result: COMPLETE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KB Hits: []\n",
      "Initial Answer: To version your APIs, use a clear and consistent strategy such as including the version number in the URL (e.g., `/api/v1/resource`) or in the request header. Ensure backward compatibility for existing clients and document changes thoroughly. Semantic versioning (e.g., major.minor.patch) is recommended to indicate the scope of changes. Major versions should be used for breaking changes, minor for new features, and patch for bug fixes. [KB004] [KB007]\n",
      "Critique: COMPLETE\n",
      "Answer was not refined.\n",
      "{'answer': 'To version your APIs, use a clear and consistent strategy such as including the version number in the URL (e.g., `/api/v1/resource`) or in the request header. Ensure backward compatibility for existing clients and document changes thoroughly. Semantic versioning (e.g., major.minor.patch) is recommended to indicate the scope of changes. Major versions should be used for breaking changes, minor for new features, and patch for bug fixes. [KB004] [KB007]'}\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I version my APIs?\"\n",
    "response = agentic_rag_qa(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fe951b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 10:26:12,033 | INFO | Embedding text for retrieval: What should I consider for error handling?\n",
      "2025-06-11 10:26:13,025 | INFO | HTTP Request: POST https://eastus.api.cognitive.microsoft.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-02-01 \"HTTP/1.1 200 OK\"\n",
      "2025-06-11 10:26:13,027 | INFO | Querying Weaviate for KB hits.\n",
      "2025-06-11 10:26:15,029 | INFO | Querying Weaviate for KB hits.\n",
      "2025-06-11 10:26:17,030 | INFO | Querying Weaviate for KB hits.\n",
      "2025-06-11 10:26:17,030 | ERROR | KB retrieval failed: RetryError[<Future at 0x792eaeb5e5a0 state=finished raised TypeError>]\n",
      "2025-06-11 10:26:17,033 | INFO | Calling LLM with prompt: \n",
      "You are a software best-practices assistant.\n",
      "User Question:\n",
      "What should I consider for error handli...\n",
      "2025-06-11 10:26:19,005 | INFO | HTTP Request: POST https://swedencentral.api.cognitive.microsoft.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-06-11 10:26:19,006 | INFO | Initial answer: To ensure effective error handling, consider the following:\n",
      "\n",
      "1. **Use clear and descriptive error messages** to help users and developers understand the issue and how to resolve it [KB001].\n",
      "2. **Log errors appropriately** to aid debugging and monitoring, ensuring sensitive information is excluded [KB002].\n",
      "3. **Implement graceful degradation** to maintain partial functionality when errors occur [KB003].\n",
      "4. **Avoid swallowing exceptions**; instead, propagate or handle them meaningfully [KB004].\n",
      "5. **Validate inputs rigorously** to prevent errors caused by invalid data [KB005].\n",
      "6. **Test error scenarios** thoroughly to ensure robustness [KB006].\n",
      "2025-06-11 10:26:19,007 | INFO | Calling LLM with prompt: \n",
      "You are a critical QA assistant. The user asked: What should I consider for error handling?\n",
      "Initial...\n",
      "2025-06-11 10:26:19,392 | INFO | HTTP Request: POST https://swedencentral.api.cognitive.microsoft.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-06-11 10:26:19,393 | INFO | Critique result: COMPLETE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KB Hits: []\n",
      "Initial Answer: To ensure effective error handling, consider the following:\n",
      "\n",
      "1. **Use clear and descriptive error messages** to help users and developers understand the issue and how to resolve it [KB001].\n",
      "2. **Log errors appropriately** to aid debugging and monitoring, ensuring sensitive information is excluded [KB002].\n",
      "3. **Implement graceful degradation** to maintain partial functionality when errors occur [KB003].\n",
      "4. **Avoid swallowing exceptions**; instead, propagate or handle them meaningfully [KB004].\n",
      "5. **Validate inputs rigorously** to prevent errors caused by invalid data [KB005].\n",
      "6. **Test error scenarios** thoroughly to ensure robustness [KB006].\n",
      "Critique: COMPLETE\n",
      "Answer was not refined.\n",
      "{'answer': 'To ensure effective error handling, consider the following:\\n\\n1. **Use clear and descriptive error messages** to help users and developers understand the issue and how to resolve it [KB001].\\n2. **Log errors appropriately** to aid debugging and monitoring, ensuring sensitive information is excluded [KB002].\\n3. **Implement graceful degradation** to maintain partial functionality when errors occur [KB003].\\n4. **Avoid swallowing exceptions**; instead, propagate or handle them meaningfully [KB004].\\n5. **Validate inputs rigorously** to prevent errors caused by invalid data [KB005].\\n6. **Test error scenarios** thoroughly to ensure robustness [KB006].'}\n"
     ]
    }
   ],
   "source": [
    "question = \"What should I consider for error handling?\"\n",
    "response = agentic_rag_qa(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b5e5f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
