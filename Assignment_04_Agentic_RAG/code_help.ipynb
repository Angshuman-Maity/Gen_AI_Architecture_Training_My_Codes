# --- 1. Imports and Setup ---

import os
import re
from typing import List, Dict, Any
import weaviate
from openai import OpenAI
from dotenv import load_dotenv

# LangGraph core imports
from langgraph.graph import StateGraph, State, END

# Load envs, clients (reuse your setup)
load_dotenv()
weaviate_url = os.environ["WEAVIATE_URL"]
weaviate_api_key = os.environ["WEAVIATE_API_KEY"]
base_url = os.getenv("MODEL_BASE_URL")
api_key = os.getenv("AZURE_OPENAI_API_KEY")
api_version = os.getenv("AZURE_OPENAI_API_VERSION")

weaviate_client = weaviate.connect_to_weaviate_cloud(
    cluster_url=weaviate_url,
    auth_credentials=weaviate.classes.init.Auth.api_key(weaviate_api_key),
)
collection = weaviate_client.collections.get("Docs")

embedding_client = OpenAI(
    api_key=api_key,
    base_url=base_url,
    default_headers={"api-key": api_key},
    default_query={"api-version": api_version}
)

# LLM setup (Azure OpenAI, gpt-4 or gpt-35-turbo, temperature=0)
llm_client = embedding_client
llm_deployment = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME")  # Set your deployment name for completion

def llm(prompt: str, temperature: float = 0) -> str:
    completion = llm_client.chat.completions.create(
        model=llm_deployment,  # e.g., 'gpt-4' or your deployment name
        messages=[{"role": "user", "content": prompt}],
        temperature=temperature,
        max_tokens=512,
        stop=None
    )
    return completion.choices[0].message.content.strip()

# --- 2. LangGraph Node Logic ---

def retrieve_kb_node(state):
    user_question = state["user_question"]
    # Embed the query
    query_vector = embedding_client.embeddings.create(
        input=user_question, model="text-embedding-ada-002"
    ).data[0].embedding
    # Query Weaviate with the vector
    results = collection.query.near_vector(
        near=query_vector,
        limit=5,
        return_metadata=["doc_id", "source", "last_updated"],
        return_properties=["doc_id", "source", "last_updated"],
    )
    kb_hits = []
    for res in results.objects:
        kb_hits.append({
            "doc_id": res.properties.get("doc_id"),
            "answer_snippet": res.properties.get("answer_snippet", ""),  # You may need to store snippet as property
            "source": res.properties.get("source", "")
        })
    return {"user_question": user_question, "kb_hits": kb_hits}

def generate_answer_node(state):
    user_question = state["user_question"]
    kb_hits = state["kb_hits"]
    kb_snippets = "\n".join(
        f"[{hit['doc_id']}] {hit['answer_snippet']}" for hit in kb_hits
    )
    prompt = f"""
You are a software best-practices assistant.
User Question:
{user_question}
Retrieved Snippets:
{kb_snippets}
Task:
Based on these snippets, write a concise answer to the userâ€™s question.
Cite each snippet you use by its doc_id in square brackets (e.g., [KB004]).
Return only the answer text.
"""
    initial_answer = llm(prompt)
    return {**state, "initial_answer": initial_answer}

def critique_answer_node(state):
    user_question = state["user_question"]
    initial_answer = state["initial_answer"]
    kb_hits = state["kb_hits"]
    kb_snippets = "\n".join(
        f"[{hit['doc_id']}] {hit['answer_snippet']}" for hit in kb_hits
    )
    prompt = f"""
You are a critical QA assistant. The user asked: {user_question}
Initial Answer:
{initial_answer}
KB Snippets:
{kb_snippets}
Task:
Determine if the initial answer fully addresses the question using only these snippets.
- If it does, respond exactly: COMPLETE
- If it misses any point or cites missing info, respond: REFINE: <short list of missing topic keywords>
Return exactly one line.
"""
    critique_result = llm(prompt)
    return {**state, "critique_result": critique_result}

def refine_answer_node(state):
    user_question = state["user_question"]
    initial_answer = state["initial_answer"]
    critique_result = state["critique_result"]
    kb_hits = state["kb_hits"]
    # Extract missing topic keywords
    match = re.match(r"REFINE:\s*(.*)", critique_result)
    missing_keywords = match.group(1).strip() if match else ""
    # New query
    new_query = f"{user_question} and information on {missing_keywords}"
    # Embed new query
    new_query_vector = embedding_client.embeddings.create(
        input=new_query, model="text-embedding-ada-002"
    ).data[0].embedding
    results = collection.query.near_vector(
        near=new_query_vector,
        limit=1,
        return_metadata=["doc_id", "source", "last_updated"],
        return_properties=["doc_id", "source", "last_updated"],
    )
    if results.objects:
        extra_hit = results.objects[0]
        extra_hit_doc_id = extra_hit.properties.get("doc_id")
        extra_hit_snippet = extra_hit.properties.get("answer_snippet", "")
    else:
        extra_hit_doc_id = ""
        extra_hit_snippet = ""
    prompt = f"""
You are a software best-practices assistant refining your answer. The user asked: {user_question}
Initial Answer:
{initial_answer}
Critique: {critique_result}
Additional Snippet:
[{extra_hit_doc_id}] {extra_hit_snippet}
Task:
Incorporate this snippet into the answer, covering the missing points.
Cite any snippet you use by doc_id in square brackets.
Return only the final refined answer.
"""
    refined_answer = llm(prompt)
    return {**state, "refined_answer": refined_answer}

# --- 3. LangGraph Graph Definition ---

class RAGState(State):
    user_question: str
    kb_hits: List[Dict]
    initial_answer: str = ""
    critique_result: str = ""
    refined_answer: str = ""

graph = StateGraph(RAGState)
graph.add_node("retrieve_kb", retrieve_kb_node)
graph.add_node("generate_answer", generate_answer_node)
graph.add_node("critique_answer", critique_answer_node)
graph.add_node("refine_answer", refine_answer_node)
graph.add_node(END, lambda state: state)

graph.add_edge("retrieve_kb", "generate_answer")
graph.add_edge("generate_answer", "critique_answer")

def critique_decision(state):
    critique_result = state["critique_result"]
    if critique_result.strip() == "COMPLETE":
        return END
    elif critique_result.strip().startswith("REFINE"):
        return "refine_answer"
    else:
        return END

graph.add_conditional_edges("critique_answer", critique_decision, {"refine_answer", END})
graph.add_edge("refine_answer", END)

graph.set_entry_point("retrieve_kb")
graph.set_exit_point(END)

# --- 4. Pipeline Driver Function ---

def agentic_rag_qa(user_question: str):
    initial_state = {
        "user_question": user_question,
        "kb_hits": [],
        "initial_answer": "",
        "critique_result": "",
        "refined_answer": ""
    }
    final_state = graph.run(initial_state)
    # Choose answer depending on flow
    answer = final_state.get("refined_answer") or final_state.get("initial_answer")
    return {"answer": answer}

# --- 5. Example Usage ---

# Example: Replace with your test question
question = "How can I prevent race conditions in Python multithreading?"
response = agentic_rag_qa(question)
print(response)
